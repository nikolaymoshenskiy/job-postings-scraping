{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job categories list\n",
    "categories = ['SERVICE_INDUSTRY', 'INFORMATION_TECHNOLOGY', 'TRADE', 'STATE_PUBLIC_ADMIN', 'EDUCATION_SCIENCE', 'SALES', 'ORGANISATION_MANAGEMENT', 'PRODUCTION_MANUFACTURING', 'ADMINISTRATION',\n",
    "'LOGISTICS_TRANSPORT', 'FINANCE_ACCOUNTING', 'TOURISM_HOTELS_CATERING', 'TECHNICAL_ENGINEERING', 'CONSTRUCTION_REAL_ESTATE', 'HEALTH_SOCIAL_CARE', 'BANKING_INSURANCE', 'ELECTRONICS_TELECOM',\n",
    "'ENERGETICS_ELECTRICITY', 'MARKETING_ADVERTISING', 'SECURITY_RESCUE_DEFENCE', 'LAW_LEGAL', 'OTHER', 'CULTURE_ENTERTAINMENT', 'MEDIA_PR', 'HUMAN_RESOURCES', 'QUALITY_ASSURANCE',\n",
    "'AGRICULTURE_ENVIRONMENTAL', 'FOREST_WOODCUTTING', 'INTERNSHIP', 'THIRD_SECTOR_NGO', 'PHARMACY', 'SEASONAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of first pages for each category\n",
    "first_pages_list = []\n",
    "for i in categories:\n",
    "    first_pages_list.append(f\"https://cv.ee/en/search?limit=20&offset=0&categories%5B0%5D={i}\")\n",
    "first_pages_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find total number of jobs and create a list of pages' URLs\n",
    "all_pages_list = []\n",
    "\n",
    "for url in first_pages_list:\n",
    "\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, features='html')\n",
    "\n",
    "    total_jobs = soup.find('div', class_ = 'search-results-heading').text\n",
    "    total_jobs_number =  int((re.findall(r'\\d+', total_jobs))[0])\n",
    "\n",
    "    url_values_range = np.arange(0, total_jobs_number, 20)\n",
    "\n",
    "    for i in url_values_range:\n",
    "        all_pages_list.append(f\"https://www.cv.ee/en/search?limit=20&offset={i}&categories%5B0%5D={url.rsplit('=', 1)[1]}\")\n",
    "\n",
    "all_pages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of jobs' basic information from all pages\n",
    "jobs_list = []\n",
    "\n",
    "for page in all_pages_list:\n",
    "    response = requests.get(page)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, features='html')\n",
    "    jobs = soup.find('ul', class_ = 'vacancies-list')\n",
    "\n",
    "    for job in jobs:\n",
    "        title = job.find(class_ = 'vacancy-item__title').text\n",
    "        location = job.find(class_ = 'vacancy-item__locations').text\n",
    "        company = job.find(class_ = 'vacancy-item__body').find('a', class_ = \"jsx-145194818\").text\n",
    "\n",
    "        ### Find url\n",
    "        url = job.find_all('a', class_ = 'jsx-145194818 vacancy-item')\n",
    "        url_list = []\n",
    "        for i in url:\n",
    "            url_list.append('https://www.cv.ee' + i.attrs['href'])\n",
    "    \n",
    "        jobs_list.append({\n",
    "            'title': title,\n",
    "            'location': location,\n",
    "            'company': company,\n",
    "            'url': url_list[0],\n",
    "            'category': page.rsplit('=', 1)[1]\n",
    "        }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a job can be in multiple categories the number of records is higher than the actual total job offers\n",
    "print(len(jobs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create jobs dataframe\n",
    "df = pd.DataFrame(jobs_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add job IDs from the URLs\n",
    "id_list = []\n",
    "\n",
    "for i in df['url']:\n",
    "    id_list.append((str(i)).split('/')[5])\n",
    "\n",
    "df['id'] = id_list\n",
    "df['id'] = df['id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group df by category and put categories to a list\n",
    "df = df.groupby(['id', 'title', 'company', 'location', 'url'])['category'].apply(list).reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe len should be equal to the actual number of job offers\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of URLs to get additional information\n",
    "url_list = df['url'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get additional information about job vacancies (deadline, work type, language and salary)   \n",
    "work_deadline = []\n",
    "work_type = []\n",
    "work_lang= []\n",
    "work_salary = []\n",
    "\n",
    "for url in url_list:\n",
    "\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, features='html')\n",
    "\n",
    "    # Deadline\n",
    "    deadlines = soup.find_all(class_= 'jsx-4256297253 vacancy-info__deadline')\n",
    "    deadline_list = []\n",
    "    try:\n",
    "        for i in deadlines:\n",
    "            deadline_list.append((i.text.replace(u'\\xa0', u'')))\n",
    "        work_deadline.append(deadline_list[0]) \n",
    "    except IndexError:\n",
    "        work_deadline.append(None)\n",
    "\n",
    "    # Language\n",
    "    languages = soup.find_all('li', class_= 'jsx-930987492')\n",
    "    lang_list = []\n",
    "    for i in languages:\n",
    "        lang_list.append(i.text.replace(u'\\xa0', u''))\n",
    "\n",
    "    work_lang.append(lang_list)\n",
    "\n",
    "    # Work type                            \n",
    "    type_list = []\n",
    "    types = soup.find_all('ul', class_= 'jsx-930987492 vacancy-highlights__section-list')\n",
    "    try:\n",
    "        if len(types) == 2 and len(lang_list) > 0:\n",
    "            for i in types[0]:\n",
    "                type_list.append(i.text.replace(u'\\xa0', u''))\n",
    "        else:\n",
    "            for i in types[1]:\n",
    "                type_list.append(i.text.replace(u'\\xa0', u'')) \n",
    "    except IndexError: \n",
    "        type_list.append(None)\n",
    "\n",
    "    work_type.append(type_list)\n",
    "\n",
    "    # Salary\n",
    "    salary = soup.find_all(class_= 'jsx-930987492 vacancy-highlights__salary-amount')\n",
    "    salaries_list = []\n",
    "    try:\n",
    "        for i in salary:\n",
    "            salaries_list.append(((i.get_text(strip=True, separator='\\n').splitlines()))[-1])\n",
    "        work_salary.append(salaries_list[0])\n",
    "    except IndexError: \n",
    "        work_salary.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append values to the dataframe\n",
    "df['language'] = work_lang\n",
    "df['work_type'] = work_type\n",
    "df['deadline'] = work_deadline\n",
    "df['salary'] = work_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split location into three columns for further analysis\n",
    "df['location_rev'] = df['location'].str.replace(',', '')\n",
    "df['location_rev'] = df['location_rev'].str.split().apply(reversed).apply(' '.join)\n",
    "df[['country', 'region', 'city']] = df['location_rev'].str.split(expand=True)\n",
    "df.drop(columns=['location_rev'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert deadline to datetime format\n",
    "df['deadline'] = ((df['deadline'].str.split(':').str[-1]).str.slice(0, 11))\n",
    "df['deadline'] = pd.to_datetime(df['deadline'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first number of the salary and convert to numeric\n",
    "df['salary'].str.split('-', expand=True)[0]\n",
    "df['salary']= pd.to_numeric(df['salary'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categories dummies as a separate df\n",
    "category = df['category']\n",
    "df_cat = pd.get_dummies(category.explode()).groupby(level=0).sum()\n",
    "df_categories = df[['id']].join(df_cat)\n",
    "df_categories.columns = df_categories.columns.str.lower()\n",
    "df_categories.columns = df_categories.columns.str.title()\n",
    "df_categories = df_categories.rename(columns=lambda name: name.replace('_', ' ')) \n",
    "df_categories     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get language dummies as a separate df\n",
    "language = df['language']\n",
    "df_lang = pd.get_dummies(language.explode()).groupby(level=0).sum()\n",
    "df_languages = df[['id']].join(df_lang)\n",
    "df_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get work type dummies as a separate df\n",
    "worktype = df['work_type']\n",
    "df_worktype = pd.get_dummies(worktype.explode()).groupby(level=0).sum()\n",
    "df_worktype = df[['id']].join(df_worktype)\n",
    "df_worktype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns from the main dataframe\n",
    "df.drop(columns=['category', 'language', 'work_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframes for further visualisation\n",
    "df.to_excel(os.path.join('output', 'jobs_main.xlsx'))\n",
    "df_categories.to_excel(os.path.join('output', 'jobs_categories.xlsx'))\n",
    "df_languages.to_excel(os.path.join('output', 'jobs_languages.xlsx'))\n",
    "df_worktype.to_excel(os.path.join('output', 'jobs_worktype.xlsx'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
